# 6.6 本章实战练习

本节通过递进式的练习，帮助读者掌握思维链（Chain of Thought, CoT）提示的设计与应用。

## 练习一：数学应用题（基础通用）

**目标**：掌握标准的思维链提示构建方法。

**任务描述**：
解决一个多步骤的数学应用题，这类题目直接回答容易出错。

**题目**：
"小明有 5 个苹果。小红的苹果数量是小明的 3 倍少 2 个。小华的苹果数量是小红的一半多 1 个。请问他们三个人一共有多少个苹果？"

**步骤**：
1. **直接问答**：直接将题目发送给模型，不使用任何引导词，记录答案。
2. **零样本 CoT**：在问题后加上 "让我们一步一步思考"（Let's think step by step），并要求“用 3-5 条要点列出关键步骤/核对点 + 最终答案”，对比答案及其准确性与输出长度。
3. **少样本 CoT**：编写一个类似的数学题作为示例，在示例中展示解题步骤（可限制为关键步骤），然后测试原题。

**观察**：
- 加上思维链触发词后，模型的输出长度发生了什么变化？
- “关键步骤要点”与“冗长推理过程”相比，哪种更有助于纠错且更易于在产品中复用？

---

## 练习二：逻辑谜题与反事实推理（进阶技巧）

**目标**：利用思维链处理反直觉或复杂的逻辑推理问题。

**任务描述**：
解决一个包含反常识设定的逻辑问题。

**题目**：
"假设：如果今天下雨，那么草地是干燥的（反常识设定）。如果草地是干燥的，那么我们要去野餐。现实是：今天正在下大雨。问题：我们今天去野餐吗？"

**挑战**：
模型往往受现实世界常识（下雨 -> 草地湿 -> 不去野餐）干扰，难以严格遵循题目给定的逻辑规则。

**练习要求**：
- 设计一个 CoT 提示词，明确要求模型：
    1. 仅依据给定的假设进行推理，忽略现实常识。
    2. 列出推理链条：A -> B, B -> C, A is true -> C is true。
- 验证模型是否能得出"去野餐"这个符合题目逻辑但违反常识的结论。

---

## 练习三：财务报表稽核模拟（实战场景）

**目标**：在专业领域任务中，利用 CoT 进行自我核查与分析。

**场景背景**：
你是一个财务审计助手，需要检查简报中的数据是否内部一致。

**输入数据**：
"2023年Q1公司总收入为1000万元，其中产品A收入600万元，产品B收入300万元，其他业务收入100万元。总支出为800万元，因此Q1净利润为250万元。"（注意：这里有一个隐藏的计算错误，1000-800=200，或者分项加和是否等于总收入等）

**任务**：
编写一个 CoT 提示词，要求模型执行以下步骤：
1. **提取数据**：列出所有提及的财务数字。
2. **验证加和**：计算分项收入之和是否等于总收入。
3. **验证利润**：计算 收入 - 支出 是否等于文中声称的利润。
4. **输出结论**：指出报告中存在的具体逻辑或计算错误。

**代码实现思路（系统提示词示例）**：

```markdown
你是一位细致的财务审计师。请按照以下步骤检查提供的财务简报：

步骤1 [数据提取]：提取所有具体的财务数字。
步骤2 [一致性检查]：检查各部分之和是否等于总数（如分项收入 vs 总收入）。
步骤3 [逻辑验证]：验证利润计算公式（收入 - 成本 = 利润）。
步骤4 [最终报告]：如果发现任何计算错误或矛盾，请明确指出；如果无误，输出“通过”。

请分析以下文本：
{input_text}
```

**思考**：
- 这种分步骤的指令如何防止模型"幻觉"出错误的审计结果？
- 尝试去掉步骤引导，直接问"这段文本有错误吗？"，对比两者的效果。

## 参考实现代码

以下是本章练习的参考 Python 实现。

### 1. 简单的思维链示例 (Simple CoT)
`simple_cot.py`

```python
def mock_llm_call(prompt):
    print(f"\n[Sending to LLM]...\n{prompt}\n-------------------")
    
    # 模拟 Direct 模式 (往往出错)
    if "一步一步" not in prompt:
         return "答案：23 - 15 + 12 = 8 + 12 = 20？不对，好像是... 30？"
    
    # 模拟 CoT 模式 (推理清晰)
    else:
        return """
让我们一步一步思考：
1. 初始苹果数量：23个
2. 卖出15个后：23 - 15 = 8个
3. 进货12个后：8 + 12 = 20个

答案：商店现在有20个苹果。
"""

def simple_cot_demo():
    question = "商店有23个苹果，卖出了15个，又进货了12个，现在有多少个？"
    
    print("=== Direct Prompt ===")
    print(f"Result: {mock_llm_call(question)}")
    
    print("\n=== CoT Prompt ===")
    cot_prompt = question + "\n请一步一步思考，然后给出答案。"
    print(f"Result: {mock_llm_call(cot_prompt)}")

if __name__ == "__main__":
    simple_cot_demo()
```

### 2. 自我一致性演示 (Self-Consistency)
`self_consistency.py`

```python
from collections import Counter

def mock_llm_call_varied(prompt):
    # 模拟多次生成的不同结果 (针对一个稍难的概率题)
    # 假设正确答案是 A，偶尔模型会错选 B
    import random
    r = random.random()
    if r < 0.6:
        return "经过计算... 答案是 5。"
    elif r < 0.8:
        return "推理如下... 答案是 5。"
    else:
        return "计算过程... 答案是 6。" # 错误答案

def self_consistency_demo():
    print("=== Self-Consistency Demo ===")
    prompt = "一个复杂数学推理题..."
    
    answers = []
    print("Running 5 paths...")
    for i in range(5):
        ans = mock_llm_call_varied(prompt)
        # 简单提取数字作为答案 (实际需用正则)
        final_ans = "5" if "5" in ans else "6" 
        answers.append(final_ans)
        print(f"Path {i+1}: {ans} -> Extracted: {final_ans}")
        
    # 多数投票
    count = Counter(answers)
    most_common = count.most_common(1)[0]
    print(f"\nFinal Answer (Majority Vote): {most_common[0]} (Confidence: {most_common[1]}/5)")

if __name__ == "__main__":
    self_consistency_demo()
```
