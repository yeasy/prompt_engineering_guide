# 第二章：大语言模型基础

## 2.3 模型参数与输出控制

除了提示词本身，大语言模型还提供了一系列可调参数来控制输出的特性。理解这些参数的作用，可以更精细地调控模型行为，获得更符合需求的结果。

### Temperature（温度）

Temperature 是最常用的模型参数，控制输出的随机性和创造性。

#### 工作原理

在生成每个 Token 时，模型会计算词汇表中所有可能 Token 的概率分布。Temperature 参数影响这个分布的"锐度"：

- **Temperature = 0**：模型几乎总是选择概率最高的 Token，输出高度确定性
- **Temperature = 1**：按原始概率分布采样
- **Temperature > 1**：概率分布更平坦，低概率 Token 有更多机会被选中
- **Temperature < 1**：概率分布更尖锐，高概率 Token 更容易被选中

```
示例：完成句子"人工智能的未来是..."

Temperature = 0: "人工智能的未来是光明的。"（每次相同）
Temperature = 0.7: "人工智能的未来是充满可能性的。"（略有变化）
Temperature = 1.2: "人工智能的未来是一场璀璨的星际探险。"（更具创意）
```

#### 使用建议

| 场景 | 推荐 Temperature | 原因 |
|------|-----------------|------|
| 事实性问答 | 0 - 0.3 | 需要准确一致的答案 |
| 代码生成 | 0 - 0.2 | 代码需要精确正确 |
| 商业写作 | 0.3 - 0.5 | 平衡创意与专业性 |
| 创意写作 | 0.7 - 1.0 | 需要多样化表达 |
| 头脑风暴 | 0.8 - 1.2 | 需要突破常规的想法 |

### Top-p（核采样）

Top-p（也称为 Nucleus Sampling）是另一种控制随机性的方法，与 Temperature 互补使用。

#### 工作原理

Top-p 设定一个概率阈值，模型只从累积概率达到该阈值的最小 Token 集合中采样：

- **Top-p = 0.1**：只从最可能的少数 Token 中选择
- **Top-p = 0.9**：从覆盖 90% 概率质量的 Token 中选择
- **Top-p = 1.0**：考虑所有可能的 Token

```
假设预测分布：
"快乐" 40% | "高兴" 25% | "开心" 20% | "愉快" 10% | 其他 5%

Top-p = 0.65: 只从 "快乐"(40%) + "高兴"(25%) 中选择
Top-p = 0.90: 从 "快乐" + "高兴" + "开心" 中选择
```

#### 使用建议

- Top-p 和 Temperature 通常不需要同时调整
- 推荐做法：固定其中一个，调整另一个
- 常见组合：Temperature = 1 + Top-p 调整，或 Top-p = 1 + Temperature 调整

### Top-k

Top-k 是更简单的采样限制方法，只从概率最高的 k 个 Token 中采样。

- **Top-k = 1**：贪婪解码，总是选最高概率
- **Top-k = 50**：从 top 50 个候选中采样

Top-k 的问题在于它不考虑概率分布的实际形状，在某些情况下可能排除合理选项或包含不合理选项。因此现代实践中 Top-p 更为常用。

### Max Tokens（最大 Token 数）

Max Tokens 设定模型输出的最大长度限制。

#### 注意事项

- 这是输出的**上限**，不是固定长度
- 如果任务需要更多内容，模型可能在达到限制时突然截断
- 需要在成本控制和内容完整性之间权衡

#### 合理设置

```
任务类型        建议 Max Tokens
短答案问答      50 - 200
摘要生成        200 - 500
文章写作        500 - 2000
代码生成        500 - 4000
长文档处理      根据需要设置更高
```

### 停止序列（Stop Sequences）

停止序列告诉模型在生成特定字符串时停止输出。

#### 应用场景

**格式控制**：
```
提示词：请列出三个要点，每个要点一行。

停止序列：["4.", "四、"]  // 防止生成超过三个要点
```

**结构化输出**：
```
提示词：生成一个 JSON 对象...

停止序列：["}"]  // 在 JSON 结束时停止（需谨慎使用）
```

**多轮对话**：
```
停止序列：["User:", "Human:"]  // 防止模型模拟用户输入
```

### Presence Penalty（存在惩罚）

Presence Penalty 减少模型重复已经出现过的 Token 的倾向。

- **正值**：鼓励模型涉及新话题，减少重复
- **负值**：允许更多重复（少见）
- **取值范围**：通常 -2.0 到 2.0

适用场景：
- 需要多样化内容时增加此值
- 撰写涉及广泛话题的文章

### Frequency Penalty（频率惩罚）

Frequency Penalty 根据 Token 在当前输出中的出现次数进行惩罚，出现越多，惩罚越重。

与 Presence Penalty 的区别：
- Presence Penalty：只看是否出现过（二元）
- Frequency Penalty：按出现次数累积惩罚

适用场景：
- 避免同一个词反复出现
- 需要词汇多样性的写作任务

### 参数组合策略

针对不同任务类型，以下是推荐的参数组合：

#### 精确型任务（事实问答、代码、数据提取）

```json
{
  "temperature": 0,
  "top_p": 1,
  "presence_penalty": 0,
  "frequency_penalty": 0
}
```

#### 平衡型任务（商业写作、技术文档）

```json
{
  "temperature": 0.5,
  "top_p": 0.9,
  "presence_penalty": 0.3,
  "frequency_penalty": 0.3
}
```

#### 创意型任务（故事创作、头脑风暴）

```json
{
  "temperature": 0.9,
  "top_p": 0.95,
  "presence_penalty": 0.6,
  "frequency_penalty": 0.5
}
```

### 参数调优流程

```
1. 确定任务类型
   ↓
2. 选择初始参数组合
   ↓
3. 对相同提示词运行多次
   ↓
4. 评估输出质量和一致性
   ↓
5. 微调单个参数
   ↓
6. 重复测试直到满意
```

### 与提示词设计的协同

参数设置和提示词设计应该协同考虑：

**互补关系**：
- 清晰的提示词可以降低对参数调整的依赖
- 好的参数设置可以增强提示词的效果

**冲突避免**：
- 如果提示词要求"发挥创意"，但 Temperature 设为 0，效果会受影响
- 如果提示词要求"精确回答"，但 Temperature 过高，可能导致不稳定

**综合考虑**：
```
任务需求 → 提示词设计 + 参数选择 → 测试验证 → 迭代优化
```

### 小结

模型参数提供了提示词之外的另一个输出控制维度。Temperature 和 Top-p 控制随机性，Max Tokens 控制长度，惩罚参数控制重复性。理解并善用这些参数，配合良好的提示词设计，可以更精确地获得期望的输出效果。实际应用中，建议从推荐的参数组合开始，通过测试逐步调优。

### 延伸阅读

- [OpenAI API Reference](https://platform.openai.com/docs/api-reference/chat/create) - OpenAI 参数完整说明
- [Anthropic API Reference](https://docs.anthropic.com/en/api/messages) - Claude 参数完整说明
- [The Curious Case of Temperature in LLMs](https://www.lesswrong.com/posts/t5LGHMGQ7Gxq6TbJ4/the-curious-case-of-neural-network-test-time-training) - 温度参数深入解析
