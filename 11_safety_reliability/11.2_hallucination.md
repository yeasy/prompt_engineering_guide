# 第十一章：安全性与可靠性

## 11.2 幻觉问题与事实性保障

幻觉（Hallucination）是指模型生成看似合理但实际错误的信息。本节介绍识别和减少幻觉的方法。

### 幻觉的类型

- **事实错误**：错误的数据、日期、人名
- **虚构内容**：不存在的引用、文献
- **逻辑矛盾**：前后矛盾的陈述
- **过度自信**：对不确定的事项过于肯定

### 减少幻觉的策略

#### 提示词层面

```
回答要求：
1. 只使用提供的参考资料
2. 对于不确定的信息标注"不确定"
3. 不要猜测或编造信息
4. 如果无法回答，明确说明
```

#### 系统层面

- 使用RAG接入可靠知识源
- 要求模型引用来源
- 实施事实核查流程
- 多模型交叉验证

### 检测幻觉

```
核查提示词：
请评估以下回答的事实准确性：

回答内容：{response}

检查项：
1. 是否有可验证的错误？
2. 是否有未注明来源的断言？
3. 是否过于确定地表述不确定的事项？
```

### 小结

幻觉是大语言模型的固有挑战，主要表现为事实错误、虚构内容、逻辑矛盾和过度自信。减少幻觉需要多层次的策略：在提示词层面要求模型承认不确定性、引用来源；在系统层面通过 RAG 接入可靠知识源、实施事实核查流程。对于高风险应用（如医疗、法律、金融），建议采用多模型交叉验证和人工复核机制。

理解幻觉的本质有助于设计更可靠的 AI 应用。接下来我们将探讨偏见问题和企业级安全架构。

### 延伸阅读

- [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/) - LLM 安全指南
- [Anthropic Safety](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/mitigate-jailbreaks) - 安全最佳实践
