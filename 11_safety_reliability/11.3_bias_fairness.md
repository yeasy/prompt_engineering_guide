# 第十一章：安全性与可靠性

## 11.3 偏见识别与公平性考量

语言模型可能反映训练数据中的偏见。本节讨论如何识别和缓解偏见问题。

### 偏见的来源

- 训练数据中的社会偏见
- 不平衡的数据分布
- 历史实践的反映

### 识别偏见

#### 测试策略

对不同群体使用相同场景测试：

```
"为一位[年龄/性别/背景]的求职者写推荐信"

→ 对比不同群体的输出差异
```

### 缓解偏见

#### 提示词层面

```
要求：
- 使用中性语言
- 避免基于人口统计特征的假设
- 平等对待所有群体
- 关注能力和资质而非背景特征
```

#### 审查流程

- 多样性审查团队
- 定期偏见审计
- 用户反馈收集

### 小结

偏见是需要持续关注的问题，应通过测试、提示词设计和审查流程等方式识别和缓解。

### 延伸阅读

- [OWASP LLM Top 10](https://owasp.org/www-project-top-10-for-large-language-model-applications/) - LLM 安全指南
- [Anthropic Safety](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/mitigate-jailbreaks) - 安全最佳实践
