# 12.5 本章实战练习

本节提供实战练习，帮助读者体验从手动调试迈向自动化提示词工程（Automated Prompt Engineering, APE）的过程。

## 练习一：基于 DSPy 的自动编译体验

**目标**：理解自动提示词优化框架（如 DSPy）的基本逻辑理念，告别"炼丹式"手工调优。

**场景描述**：
你有一个任务：对电商产品的英文评论进行提取，提炼出 3 个优点的关键词，并翻译成中文返回。
传统方式是：你写一段长长的指令，然后用 10 条测试数据跑一下，发现漏了格式要求，再手工加上 `Output exactly in JSON...`，再跑一遍，如此反复。

**任务**：
使用伪代码或通过思考，回答以下在 DSPy 框架中的操作对应传统提示词工程的哪一步：
1. **定义 Signature**：`Review -> Keywords_CN`
2. **定义 Metrics**：编写一个 Python 函数，检查返回的答案是否严格为 3 个中文词汇组成的列表。
3. **编译 (Compile)**：将 50 条历史优质评论连同它们的标准答案送入 DSPy 编译器（如 `BootstrapFewShot` 优化器）。

**思考**：为什么说在长期的复杂项目中，"调整评估指标"比"猜测大模型在想啥并手工修改提示词文本"更有效率？

---

## 练习二：编写鲁棒的 LLM-as-a-Judge 评估器

**目标**：掌握如何使用强模型（如 GPT-4）去自动化评估弱模型（如 Llama-3-8B）的输出。

**场景背景**：
你的客服机器人在回复用户关于退货政策的问题时，存在一定的幻觉比例。因为人工评估太慢了，你决定写一个 "LLM Judge" 提示词来取代人工质检。

**任务**：
请为 LLM Judge 设计一段 系统提示词（system prompt）。
**输入变量包含**：
- `[用户提问]`
- `[客服模型回复]`
- `[知识库标准答案]`

**设计要求**：
1. 不仅要打分，还必须输出**评分维度**（比如：准确性、礼貌度、是否包含拒绝话术）。
2. 让 LLM Judge 使用连贯的思维链—— 即强制它在给出最终分数前，先根据各维度进行解释分析。
3. **关键挑战**：如何防止 LLM Judge 本身产生幻觉或者偏袒长回答（即"冗长偏见 Verbosity Bias"）？（提示：在 Prompt 中明确说明短小精悍的正确答案应得满分）

---

## 练习三：搭建最简版 PromptOps 流水线

**目标**：梳理用于提示词版本控制与自动化测试的 CI/CD 流程逻辑。

**场景描述**：
你的团队有 3 个产品经理不断在调整"文案生成"的提示词模板。经常发生 A 修复了一个语气问题，却不小心删掉了格式约束，导致后台接口全部报错的事故。

**任务**：
设计一个简单的 Prompt 自动化测试流水线。你可以借助 GitHub Actions 的思路：
1. **版本存储**：规定提示词模板如何与代码库共存（比如统一放在 `prompts/` 文件夹下，使用 `.yaml` 或 `.jinja2` 格式管理）。
2. **触发条件**：当产品经理在 Git 上提交新的 Prompt 时。
3. **执行流程**：流水线脚本应拉取哪几类测试集数据？调用哪个测试脚本？
4. **通行标准**：通过率达到多少才可以合并入 Main 分支并发布到生产环境的配置中心？
